# NLP 
"궁극적인 목표 : 의미 추출"

어떻게 할까?
(Word, 센턴스, 임베딩)

## 오늘의 주제는 워드 임베딩!!!!
벡터로 표현한 것. 

hi hello
dog puppy 
cat tiger 

![jw.png](img/jw.png)

단어들의 방향성을 가지고, 비슷한 의미 것들은 가까이에 위치해 있다는 것. 

가구 안에 의자 테이블 옷장 등 
![many.png](img/manytomany.PNG)

![relation.png](img/relation.PNG)

# distributional Hypothesis  
워드 벡터 공간 상에서 비슷한 것들끼리는 주변에 있어야한다. 

# word2Vec  
CBOW , Skip-Gram 

# Word similarity Task 
소년 청년 8.83의 유사도를 가짐. 
![cos.png](img/cos.PNG)

여기에서 보면 Human E~ 에서 8.83으로 정해져 있다면, 
저 사이 cos인도 8.~의 유사도를 가진다는 것을 볼 수 있고, 
수닭이랑 식품은 유사도가 낮기 때문에 낮게 나옴.  

# word Analogy Task

워드 임베딩은 제일 많이 쓰지만, 한계점도 존재한다. 
없는 단어(이미 학습되어 있지 않은 단어, input되어 있지 않은 단어)에 대해서는 학습할 수 없다.

# Subword Information skip-Gram 
where ->나눠서 기억해봄  
ex )   
wh   
whe   
wher  
her  
ere
 . . . 이런식으로. 
 
# Contextualised word 임베딩 
즉, 이중 의미를 가지는 녀석들. 
그녀는 사과했다.  
사과? -> 먹는 사과? 미안함? 
다른 임베딩을 가질 수 있도록 설계한다. 





