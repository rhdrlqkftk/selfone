# NLP 
"궁극적인 목표 : 의미 추출"

어떻게 할까?
(Word, 센턴스, 임베딩)

## 오늘의 주제는 워드 임베딩!!!!
벡터로 표현한 것. 

hi hello
dog puppy 
cat tiger 

![jw.png](img/jw.png)

단어들의 방향성을 가지고, 비슷한 의미 것들은 가까이에 위치해 있다는 것. 

가구 안에 의자 테이블 옷장 등 
![many.png](img/manytomany.PNG)

![relation.png](img/relation.PNG)

# distributional Hypothesis  
워드 벡터 공간 상에서 비슷한 것들끼리는 주변에 있어야한다. 

# word2Vec  
CBOW , Skip-Gram 

# Word similarity Task 
소년 청년 8.83의 유사도를 가짐. 
![cos.png](img/cos.PNG)

여기에서 보면 Human E~ 에서 8.83으로 정해져 있다면, 
저 사이 cos인도 8.~의 유사도를 가진다는 것을 볼 수 있고, 
수닭이랑 식품은 유사도가 낮기 때문에 낮게 나옴.  

# word Analogy Task

워드 임베딩은 제일 많이 쓰지만, 한계점도 존재한다. 
없는 단어(이미 학습되어 있지 않은 단어, input되어 있지 않은 단어)에 대해서는 학습할 수 없다.

# Subword Information skip-Gram 
where ->나눠서 기억해봄  
ex )   
wh   
whe   
wher  
her  
ere
 . . . 이런식으로. 
 
# Contextualised word 임베딩 
즉, 이중 의미를 가지는 녀석들. 
그녀는 사과했다.  
사과? -> 먹는 사과? 미안함? 
다른 임베딩을 가질 수 있도록 설계한다. 

# ELMo
한방향으로 -> 단어의 의미를 찾아내는 그런거. 

# Transformers 
기본적으로 문장에서 단어들 순서상관 없이 연관 단어들을 찾아 연결해본다. 
ex i eat ~~~~~~(단어) (단어)~~ ~ ~  ~ apple 
eat, apple이 멀리 떨어져있지만, 관련있다라고 하면 찾아낸다. 
양방향으로 찾음.

# BERT 
GPT2 (위에, 트렌스 포멀을 씀, 논술을 쓸 때 사람처럼 씀)
ELMo( 한방향 , 순서 체크 )

버트는 단어 임베딩은 똑같은데 포지션 임베딩이 추가됨(양방향..)

버트로 두개의 센턴스를 가지고 어떤 관계를 가지는지 찾음, 긍정 부정을 파악함, Tagging을 함(얜 주어야, 얜 목적어야 등등).




